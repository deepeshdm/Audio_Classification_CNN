{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_on_Urban8K.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSISppmIZPe1"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKKMvGAjZOX3",
        "outputId": "3209827b-738d-43ab-ac16-3dbe4691c9f4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D,Flatten, Dropout,BatchNormalization\n",
        "from keras.callbacks import Callback\n",
        "from keras.models import load_model\n",
        "\n",
        "print(\"All Dependencies Installed\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All Dependencies Installed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3074JRM4Ywxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca8999a-e412-4c8e-e677-2c9e3d5935c8"
      },
      "source": [
        "dataset_dir = r\"/content/drive/MyDrive/Urban8K_Dataset\"\n",
        "df = pd.read_csv(dataset_dir + r\"/UrbanSound8K.csv\")\n",
        "\n",
        "# We'll not use the full dataset to avoid running out of memory\n",
        "num_samples = 1500\n",
        "df = df.sample(n=num_samples)\n",
        "\n",
        "print(df.info())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1500 entries, 4808 to 7024\n",
            "Data columns (total 8 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   slice_file_name  1500 non-null   object \n",
            " 1   fsID             1500 non-null   int64  \n",
            " 2   start            1500 non-null   float64\n",
            " 3   end              1500 non-null   float64\n",
            " 4   salience         1500 non-null   int64  \n",
            " 5   fold             1500 non-null   int64  \n",
            " 6   classID          1500 non-null   int64  \n",
            " 7   class            1500 non-null   object \n",
            "dtypes: float64(2), int64(4), object(2)\n",
            "memory usage: 105.5+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH2rFoOAZC4f"
      },
      "source": [
        "# Takes audio file name/path & returns its Mel Spectrogram.\n",
        "def convert_audio2MelSpec(audio_file):\n",
        "    samples, sample_rate = librosa.load(audio_file, sr=None)\n",
        "    spectrogram = librosa.stft(samples)\n",
        "    sgram_mag, _ = librosa.magphase(spectrogram)\n",
        "    mel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr=sample_rate)\n",
        "    mel_spectrogram = librosa.amplitude_to_db(mel_scale_sgram, ref=np.min)\n",
        "    return mel_spectrogram\n",
        "\n",
        "# Takes 2D array & desired_shape then pads it with 0 to reshape it to desired shape.\n",
        "def apply_padding(an_array,desired_shape):\n",
        "    shape = np.shape(an_array)\n",
        "    # we'll reshape all mel_spec to largest shape present in our dataset-(128, 1501)\n",
        "    padded_array = np.zeros(desired_shape)\n",
        "    padded_array[:shape[0],:shape[1]] = an_array\n",
        "    return padded_array"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md8fE00CZGsH",
        "outputId": "cef853ab-ca9f-489f-b12e-d890b2bd83b7"
      },
      "source": [
        "\n",
        "# Finding the largest shape of mel_spectrogram in our samples.\n",
        "# Since we are converting audio to mel_spec,the no. of rows is common in all i.e 128\n",
        "# so we just need to find the largest no. of columns.\n",
        "\n",
        "def shape_generator():\n",
        "    for audio_filename, fold, label in df[[\"slice_file_name\", \"fold\", \"class\"]].values:\n",
        "        file_path = dataset_dir + \"/\" + \"fold\" + str(fold) + \"/\" + audio_filename\n",
        "        mel_spec = convert_audio2MelSpec(file_path)\n",
        "        mel_shape = list(mel_spec.shape)\n",
        "        yield mel_shape\n",
        "\n",
        "\n",
        "largest_shape = [128, 0]\n",
        "i = 0\n",
        "gen = shape_generator()\n",
        "while True:\n",
        "    try:\n",
        "        melshape = next(gen)\n",
        "        if melshape[1] > largest_shape[1]:\n",
        "            largest_shape[1] = melshape[1]\n",
        "        i += 1\n",
        "\n",
        "        # prints percentage of task completed.\n",
        "        if i % 200 == 0:\n",
        "            percent = (i / num_samples) * 100\n",
        "            print(\"{} % Task Completed...\".format(round(percent, 2)))\n",
        "        \n",
        "    except StopIteration:\n",
        "        break\n",
        "\n",
        "largest_shape = tuple(largest_shape)\n",
        "print(\"Largest Shape : \", largest_shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/filters.py:239: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
            "  \"Empty filters detected in mel frequency basis. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13.33 % Task Completed...\n",
            "26.67 % Task Completed...\n",
            "40.0 % Task Completed...\n",
            "53.33 % Task Completed...\n",
            "66.67 % Task Completed...\n",
            "80.0 % Task Completed...\n",
            "93.33 % Task Completed...\n",
            "Largest Shape :  (128, 1501)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv7rYtZCZfcs",
        "outputId": "50c1ba5f-178f-4f46-ad6b-614480644f31"
      },
      "source": [
        "\n",
        "def data_generator():\n",
        "    for audio_filename, fold, label in df[[\"slice_file_name\", \"fold\", \"class\"]].values:\n",
        "        file_path = dataset_dir + \"/\" + \"fold\" + str(fold) + \"/\" + audio_filename\n",
        "        mel_spec = convert_audio2MelSpec(file_path)\n",
        "        # padding to largest shape in dataset\n",
        "        mel_spec = apply_padding(mel_spec, largest_shape)\n",
        "        # converting 2D numpy array to 2D list\n",
        "        mel_spec = mel_spec.tolist()\n",
        "        yield mel_spec, label\n",
        "\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "gen = data_generator()\n",
        "i=0\n",
        "while True:\n",
        "    try:\n",
        "        melspec, label = next(gen)\n",
        "        x.append(melspec)\n",
        "        y.append(label)\n",
        "        i+=1\n",
        "\n",
        "        # prints percentage of task completed.\n",
        "        if i % 200 == 0:\n",
        "                percent = (i / num_samples) * 100\n",
        "                print(\"{} % Task Completed...\".format(round(percent, 2)))\n",
        "\n",
        "    except StopIteration:\n",
        "        break\n",
        "\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "print(x[:3])\n",
        "print(y[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/filters.py:239: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
            "  \"Empty filters detected in mel frequency basis. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13.33 % Task Completed...\n",
            "26.67 % Task Completed...\n",
            "40.0 % Task Completed...\n",
            "53.33 % Task Completed...\n",
            "66.67 % Task Completed...\n",
            "80.0 % Task Completed...\n",
            "93.33 % Task Completed...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ7OJCNsZmp9"
      },
      "source": [
        "y_df = pd.DataFrame(data=y)\n",
        "\n",
        "# contains all string class labels\n",
        "labels = (list(pd.get_dummies(y_df)))\n",
        "\n",
        "# one-hot-encoding labels\n",
        "y = pd.get_dummies(y).values\n",
        "\n",
        "print(labels)\n",
        "print(y[:3])\n",
        "\n",
        "np.save(\"X_Urban8K\",x)\n",
        "np.save(\"Y_Urban8K\",y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE88g140ZvNE"
      },
      "source": [
        "\"\"\"\n",
        "Conv2D however expects 4 dimensions,because it also expects the channels dimension of image,\n",
        "which in MNIST is nonexistent because it’s grayscale data and hence is 1.\n",
        "Reshaping the data, while explicitly adding the channels dimension, resolves the issue.\n",
        "The input shape a CNN accepts should be in a specific format.\n",
        "In Tensorflow,the format is (num_samples, height, width, channels)\n",
        "\"\"\"\n",
        "print(\"Before Reshaping : \",x.shape)\n",
        "largest_shape = list(largest_shape)\n",
        "x = x.reshape(x.shape[0],largest_shape[0],largest_shape[1],1)\n",
        "print(\"After Reshaping\",x.shape)\n",
        "\n",
        "# Splitting data into train & test\n",
        "x_train,x_test,y_train,y_test = \\\n",
        "    train_test_split(x,y,test_size=0.2,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYoYAU5nZyyS"
      },
      "source": [
        "## **Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIsMf70JZ4Sl"
      },
      "source": [
        "input_shape = (x.shape[1],x.shape[2],x.shape[3])\n",
        "\n",
        "# Defining the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32,kernel_size=(2,2),padding=\"same\",activation=\"relu\",input_shape=input_shape))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(64,kernel_size=(2,2),padding=\"same\",activation=\"relu\"))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(128,kernel_size=(2,2),padding=\"same\",activation=\"relu\"))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(150,activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(75,activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(y_train[0]),activation=\"softmax\"))\n",
        "\n",
        "print(\"Y_train length :\",len(y_train[0]))\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9Rd5KNkaQUX"
      },
      "source": [
        "# Custom Keras callback to stop training when certain accuracy is achieved.\n",
        "class MyThresholdCallback(Callback):\n",
        "    def __init__(self, threshold):\n",
        "        super(MyThresholdCallback, self).__init__()\n",
        "        self.threshold = threshold\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_acc = logs[\"val_accuracy\"]\n",
        "        if val_acc >= self.threshold:\n",
        "            self.model.stop_training = True\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=50,\n",
        "          callbacks=[MyThresholdCallback(0.9)],validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUNG0jHYaTUs"
      },
      "source": [
        "#Saving the model\n",
        "model.save(\"Audio_Classification_CNN\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}