{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_on_Urban8K.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSISppmIZPe1"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKKMvGAjZOX3",
        "outputId": "3209827b-738d-43ab-ac16-3dbe4691c9f4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D,Flatten, Dropout,BatchNormalization\n",
        "from keras.callbacks import Callback\n",
        "from keras.models import load_model\n",
        "\n",
        "print(\"All Dependencies Installed\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All Dependencies Installed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3074JRM4Ywxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca8999a-e412-4c8e-e677-2c9e3d5935c8"
      },
      "source": [
        "dataset_dir = r\"/content/drive/MyDrive/Urban8K_Dataset\"\n",
        "df = pd.read_csv(dataset_dir + r\"/UrbanSound8K.csv\")\n",
        "\n",
        "# We'll not use the full dataset to avoid running out of memory\n",
        "num_samples = 1500\n",
        "df = df.sample(n=num_samples)\n",
        "\n",
        "print(df.info())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1500 entries, 4808 to 7024\n",
            "Data columns (total 8 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   slice_file_name  1500 non-null   object \n",
            " 1   fsID             1500 non-null   int64  \n",
            " 2   start            1500 non-null   float64\n",
            " 3   end              1500 non-null   float64\n",
            " 4   salience         1500 non-null   int64  \n",
            " 5   fold             1500 non-null   int64  \n",
            " 6   classID          1500 non-null   int64  \n",
            " 7   class            1500 non-null   object \n",
            "dtypes: float64(2), int64(4), object(2)\n",
            "memory usage: 105.5+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH2rFoOAZC4f"
      },
      "source": [
        "# Takes audio file name/path & returns its Mel Spectrogram.\n",
        "def convert_audio2MelSpec(audio_file):\n",
        "    samples, sample_rate = librosa.load(audio_file, sr=None)\n",
        "    spectrogram = librosa.stft(samples)\n",
        "    sgram_mag, _ = librosa.magphase(spectrogram)\n",
        "    mel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr=sample_rate)\n",
        "    mel_spectrogram = librosa.amplitude_to_db(mel_scale_sgram, ref=np.min)\n",
        "    return mel_spectrogram\n",
        "\n",
        "# Takes 2D array & desired_shape then pads it with 0 to reshape it to desired shape.\n",
        "def apply_padding(an_array,desired_shape):\n",
        "    shape = np.shape(an_array)\n",
        "    # we'll reshape all mel_spec to largest shape present in our dataset-(128, 1501)\n",
        "    padded_array = np.zeros(desired_shape)\n",
        "    padded_array[:shape[0],:shape[1]] = an_array\n",
        "    return padded_array"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md8fE00CZGsH",
        "outputId": "cef853ab-ca9f-489f-b12e-d890b2bd83b7"
      },
      "source": [
        "\n",
        "# Finding the largest shape of mel_spectrogram in our samples.\n",
        "# Since we are converting audio to mel_spec,the no. of rows is common in all i.e 128\n",
        "# so we just need to find the largest no. of columns.\n",
        "\n",
        "def shape_generator():\n",
        "    for audio_filename, fold, label in df[[\"slice_file_name\", \"fold\", \"class\"]].values:\n",
        "        file_path = dataset_dir + \"/\" + \"fold\" + str(fold) + \"/\" + audio_filename\n",
        "        mel_spec = convert_audio2MelSpec(file_path)\n",
        "        mel_shape = list(mel_spec.shape)\n",
        "        yield mel_shape\n",
        "\n",
        "\n",
        "largest_shape = [128, 0]\n",
        "i = 0\n",
        "gen = shape_generator()\n",
        "while True:\n",
        "    try:\n",
        "        melshape = next(gen)\n",
        "        if melshape[1] > largest_shape[1]:\n",
        "            largest_shape[1] = melshape[1]\n",
        "        i += 1\n",
        "\n",
        "        # prints percentage of task completed.\n",
        "        if i % 200 == 0:\n",
        "            percent = (i / num_samples) * 100\n",
        "            print(\"{} % Task Completed...\".format(round(percent, 2)))\n",
        "        \n",
        "    except StopIteration:\n",
        "        break\n",
        "\n",
        "largest_shape = tuple(largest_shape)\n",
        "print(\"Largest Shape : \", largest_shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/filters.py:239: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
            "  \"Empty filters detected in mel frequency basis. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13.33 % Task Completed...\n",
            "26.67 % Task Completed...\n",
            "40.0 % Task Completed...\n",
            "53.33 % Task Completed...\n",
            "66.67 % Task Completed...\n",
            "80.0 % Task Completed...\n",
            "93.33 % Task Completed...\n",
            "Largest Shape :  (128, 1501)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv7rYtZCZfcs",
        "outputId": "50c1ba5f-178f-4f46-ad6b-614480644f31"
      },
      "source": [
        "\n",
        "def data_generator():\n",
        "    for audio_filename, fold, label in df[[\"slice_file_name\", \"fold\", \"class\"]].values:\n",
        "        file_path = dataset_dir + \"/\" + \"fold\" + str(fold) + \"/\" + audio_filename\n",
        "        mel_spec = convert_audio2MelSpec(file_path)\n",
        "        # padding to largest shape in dataset\n",
        "        mel_spec = apply_padding(mel_spec, largest_shape)\n",
        "        # converting 2D numpy array to 2D list\n",
        "        mel_spec = mel_spec.tolist()\n",
        "        yield mel_spec, label\n",
        "\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "gen = data_generator()\n",
        "i=0\n",
        "while True:\n",
        "    try:\n",
        "        melspec, label = next(gen)\n",
        "        x.append(melspec)\n",
        "        y.append(label)\n",
        "        i+=1\n",
        "\n",
        "        # prints percentage of task completed.\n",
        "        if i % 200 == 0:\n",
        "                percent = (i / num_samples) * 100\n",
        "                print(\"{} % Task Completed...\".format(round(percent, 2)))\n",
        "\n",
        "    except StopIteration:\n",
        "        break\n",
        "\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "print(x[:3])\n",
        "print(y[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/filters.py:239: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
            "  \"Empty filters detected in mel frequency basis. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "13.33 % Task Completed...\n",
            "26.67 % Task Completed...\n",
            "40.0 % Task Completed...\n",
            "53.33 % Task Completed...\n",
            "66.67 % Task Completed...\n",
            "80.0 % Task Completed...\n",
            "93.33 % Task Completed...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ7OJCNsZmp9"
      },
      "source": [
        "y_df = pd.DataFrame(data=y)\n",
        "\n",
        "# contains all string class labels\n",
        "labels = (list(pd.get_dummies(y_df)))\n",
        "\n",
        "# one-hot-encoding labels\n",
        "y = pd.get_dummies(y).values\n",
        "\n",
        "print(labels)\n",
        "print(y[:3])\n",
        "\n",
        "np.save(\"X_Urban8K\",x)\n",
        "np.save(\"Y_Urban8K\",y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE88g140ZvNE"
      },
      "source": [
        "\"\"\"\n",
        "Conv2D however expects 4 dimensions,because it also expects the channels dimension of image,\n",
        "which in MNIST is nonexistent because itâ€™s grayscale data and hence is 1.\n",
        "Reshaping the data, while explicitly adding the channels dimension, resolves the issue.\n",
        "The input shape a CNN accepts should be in a specific format.\n",
        "In Tensorflow,the format is (num_samples, height, width, channels)\n",
        "\"\"\"\n",
        "print(\"Before Reshaping : \",x.shape)\n",
        "largest_shape = list(largest_shape)\n",
        "x = x.reshape(x.shape[0],largest_shape[0],largest_shape[1],1)\n",
        "print(\"After Reshaping\",x.shape)\n",
        "\n",
        "# Splitting data into train & test\n",
        "x_train,x_test,y_train,y_test = \\\n",
        "    train_test_split(x,y,test_size=0.2,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYoYAU5nZyyS"
      },
      "source": [
        "## **Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIsMf70JZ4Sl"
      },
      "source": [
        "input_shape = (x.shape[1],x.shape[2],x.shape[3])\n",
        "\n",
        "# Defining the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32,kernel_size=(2,2),padding=\"same\",activation=\"relu\",input_shape=input_shape))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(64,kernel_size=(2,2),padding=\"same\",activation=\"relu\"))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(128,kernel_size=(2,2),padding=\"same\",activation=\"relu\"))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(150,activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(75,activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(y_train[0]),activation=\"softmax\"))\n",
        "\n",
        "print(\"Y_train length :\",len(y_train[0]))\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9Rd5KNkaQUX"
      },
      "source": [
        "# Custom Keras callback to stop training when certain accuracy is achieved.\n",
        "class MyThresholdCallback(Callback):\n",
        "    def __init__(self, threshold):\n",
        "        super(MyThresholdCallback, self).__init__()\n",
        "        self.threshold = threshold\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_acc = logs[\"val_accuracy\"]\n",
        "        if val_acc >= self.threshold:\n",
        "            self.model.stop_training = True\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=50,\n",
        "          callbacks=[MyThresholdCallback(0.9)],validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUNG0jHYaTUs"
      },
      "source": [
        "#Saving the model\n",
        "model.save(\"Audio_Classification_CNN\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}